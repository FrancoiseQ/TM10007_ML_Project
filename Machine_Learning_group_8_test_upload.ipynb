{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancoiseQ/TM10007_ML_Project/blob/main/Machine_Learning_group_8_test_upload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCxyYT_h8hx2"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPFBQOfU8luM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.stats import uniform\n",
        "from sklearn import metrics, model_selection, preprocessing, svm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.metrics import accuracy_score, recall_score, roc_curve, auc, precision_recall_curve\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, StratifiedKFold, learning_curve\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.svm import LinearSVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKkwtW8cKSwX"
      },
      "source": [
        "# Load zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFfXUKEF4teP",
        "outputId": "604a2a03-6d6e-4fe0-ab6d-56cb7313029e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n",
            "The number of samples: 827\n",
            "The number of columns: 9001\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "\n",
        "with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tm10007_ml/ecg')\n",
        "\n",
        "data = pd.read_csv('/content/tm10007_ml/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq_8U5GRInUq"
      },
      "source": [
        "# Preprocessing data\n",
        "\n",
        "Below the data is preprocessed, firstly splitting the data into a training, validation and test set. Then the data is scaled upsampled and PCA is applied. The sets are defined for their respective classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSZldrOyIwym"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    this_directory = os.path.dirname(os.path.abspath(_file_))\n",
        "    data = pd.read_csv(os.path.join(this_directory, 'ecg_data.csv'), index_col=0)\n",
        "    data = pd.DataFrame(data)\n",
        "    return data\n",
        "\n",
        "def extract_data(input_data):\n",
        "    # Create variable Y\n",
        "    Y = input_data['label']\n",
        "\n",
        "    # Create variable X\n",
        "    X = input_data.drop('label', axis=1)\n",
        "    return X, Y\n",
        "\n",
        "## Preparing data\n",
        "# Load and extract data\n",
        "X,y  = extract_data(data)\n",
        "\n",
        "# Split the dataset in train and test part\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, test_size=0.2,  random_state=42)\n",
        "X_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=42)\n",
        "\n",
        "# Fit the StandardScaler on the training data\n",
        "scaler = RobustScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform the upsampled training data, validation data and test data on the same scaler\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Upsample minority class for NN and RF\n",
        "oversample = SMOTE(random_state=42)\n",
        "X_train_upsampled , y_train_upsampled = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Upsample minority class in scaled data for SVM\n",
        "X_train_upsampled_scaled , y_train_upsampled_scaled = oversample.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Fit PCA on trainingsdata\n",
        "pca = PCA(n_components=100)\n",
        "pca.fit(X_train_upsampled_scaled)\n",
        "\n",
        "# Transform validation data and test data\n",
        "X_train_pca = pca.transform(X_train_upsampled_scaled)\n",
        "X_val_pca = pca.transform(X_val_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GcZe9y5po3e"
      },
      "source": [
        "# Neural Network (NN) hyperparameter tuning\n",
        "\n",
        "In this section, a neural network, MLPClassifier is fit to the training data. A randomized search is performed to find the optimal parameters. the upsampled datasets are used to reduce the effects of class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8haE5EApoPb",
        "outputId": "cdce920d-6349-4654-86bc-bb8d2905a9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'activation': 'relu', 'alpha': 0.0016601864044243652, 'hidden_layer_sizes': (150,), 'learning_rate_init': 0.0010997491581800289, 'solver': 'adam'}\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters distributions for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (150,), (200,)],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'alpha': uniform(0.0001, 0.01),\n",
        "    'learning_rate_init': uniform(0.0001, 0.01)\n",
        "\n",
        "}\n",
        "\n",
        "# Initialize MLPClassifier\n",
        "mlp = MLPClassifier(random_state=42, max_iter=400)\n",
        "\n",
        "# Perform randomized search with StratifiedKFold Cross-validation on X_train_upsampled\n",
        "random_search = RandomizedSearchCV(mlp, param_distributions=param_grid, n_iter=10, cv=StratifiedKFold(), random_state=42, scoring='recall')\n",
        "random_search.fit(X_train_upsampled, y_train_upsampled)\n",
        "\n",
        "# Display best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Define MLP classifier with best parameters\n",
        "best_mlp = MLPClassifier(**random_search.best_params_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9nQnM_kHasn"
      },
      "source": [
        "# Support Vector Machine (SVM) hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjRMYTA-2SHA"
      },
      "source": [
        "In this section, a SVM classifier is fit to the training dat. L1 regulization is used with different regulization parameters. A randomized search is performed using stratified K fold cross validation and recall as scoring metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBasuL5XaPJY",
        "outputId": "8141488f-6c21-4c94-b04c-ec124d525609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'C': 0.5808455408458729}\n"
          ]
        }
      ],
      "source": [
        "# Initialize SVM with L1 regularization\n",
        "svm_l1 = LinearSVC(penalty='l1', dual=False, random_state=42, max_iter=300000) # Increased the number of iterations to convergence to a solution\n",
        "\n",
        "# Define hyperparameter distribution\n",
        "param_dist = {'C': uniform(0.00001, 10 - 0.00001)}  # Define a continuous distribution for C from 0.00001 (heavy regulization) to 10 (less regulization)\n",
        "\n",
        "# Perform randomized search with StratifiedKFold Cross-validation on X_train\n",
        "random_search = RandomizedSearchCV(svm_l1, param_distributions=param_dist, cv=StratifiedKFold(), random_state=42, scoring='recall')\n",
        "random_search.fit(X_train_pca, y_train_upsampled)\n",
        "\n",
        "# Retrieve the best model\n",
        "best_svm_l1 = random_search.best_estimator_\n",
        "\n",
        "# Display best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(random_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDpxm4dx0Ukb"
      },
      "source": [
        "# Random Forest (RF) hyperparameter tuning\n",
        "This code trains a random forest classifier and tunes the hyperparameters. Running this part of the code takes about 20 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vsxKWFvHgKu"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=50, bootstrap=True, random_state=42)\n",
        "\n",
        "# Define hyperparameters distributions for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV on the training set for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist, n_iter=100, cv=StratifiedKFold(), random_state=42, scoring='recall')\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Retrieve best estimator\n",
        "best_rf_classifier = random_search.best_estimator_\n",
        "\n",
        "# Display best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(random_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6OERUOpoEMV"
      },
      "source": [
        "# Learning curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MT2SqTzPiTS"
      },
      "source": [
        "Compute separate learning curves for all fitted models to evaluate overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFyQ9ZGVoMfP"
      },
      "outputs": [],
      "source": [
        "# Create subplot for learning curves\n",
        "fig, axs = plt.subplots(1,3, figsize=(18, 5), sharey=True)\n",
        "fig.suptitle('Learning curves')\n",
        "\n",
        "# Add neural network to first plot\n",
        "train_sizes, train_scores, test_scores = learning_curve(best_mlp, X_train_upsampled, y_train_upsampled, cv=StratifiedKFold(), n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5), scoring='recall')\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "axs[0].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "axs[0].fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "axs[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training Score\")\n",
        "axs[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation Score\")\n",
        "axs[0].set_title('Neural Network')\n",
        "\n",
        "# Add SVM to second plot\n",
        "train_sizes, train_scores, test_scores = learning_curve(best_svm_l1, X_train_pca, y_train_upsampled, cv=StratifiedKFold(), scoring='recall', train_sizes=np.linspace(0.1, 1.0, 20))\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "axs[1].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
        "axs[1].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
        "axs[1].plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training Score')\n",
        "axs[1].plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-Validation Score')\n",
        "axs[1].set_title('Support Vector Machine')\n",
        "\n",
        "# Add random forest to third plot\n",
        "train_sizes, train_scores, test_scores = learning_curve(best_rf_classifier, X_train_upsampled , y_train_upsampled, cv=StratifiedKFold(), scoring='recall', train_sizes=np.linspace(0.1, 1.0, 20))\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "axs[2].fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
        "axs[2].fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
        "axs[2].plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training Score')\n",
        "axs[2].plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-Validation Score')\n",
        "axs[2].set_title('Random Forest')\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.set(xlabel='Training examples', ylabel='Score')\n",
        "    ax.label_outer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlYM96Qh5_XG"
      },
      "source": [
        "# Precision recall curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8ZZ0AlMPaFj"
      },
      "source": [
        "Compute precision recall curves for the three fitted models and add the curves to one plot. Calculate AUC to evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYP6h3uv6FSv"
      },
      "outputs": [],
      "source": [
        "# Plot specifications\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Precision Recall Curves')\n",
        "ax.set_xlabel('Recall')\n",
        "ax.set_ylabel('Precision')\n",
        "\n",
        "# Add Neural Network\n",
        "best_mlp = mlp.fit(X_train_upsampled, y_train_upsampled)\n",
        "precision, recall, _ = precision_recall_curve(y_val, best_mlp.predict_proba(X_val)[:, 1])\n",
        "ax.plot(recall, precision, color='green', label='NN (AUC = %0.2f)' % auc(recall, precision))\n",
        "\n",
        "# Add SVM\n",
        "precision, recall, _ = precision_recall_curve(y_val, best_svm_l1.decision_function(X_val_pca))\n",
        "ax.plot(recall, precision, color='red', label='SVM (AUC = %0.2f)' % auc(recall, precision))\n",
        "\n",
        "# Add RF\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, best_rf_classifier.predict_proba(X_val)[:, 1])\n",
        "ax.plot(recall, precision, color='blue', label='RF (AUC = %0.2f)' % auc(recall, precision))\n",
        "\n",
        "# Show plot\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5EXkvKWJDiV"
      },
      "source": [
        "# Test on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEEulxmMJlKq"
      },
      "source": [
        "Test the chosen model on the test data and calculate accuracy and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSWArGTtJp7E"
      },
      "outputs": [],
      "source": [
        "# Predictions on test data using neural network\n",
        "y_pred = best_mlp.predict(X_test)\n",
        "\n",
        "# Compute accuracy and recall\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Recall:\", recall)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}